{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model v2 - Optimized Expanding Window\n",
    "\n",
    "This notebook implements an optimized ARIMA model with expanding window validation for predicting stock returns. This version is designed for memory efficiency and performance with large datasets while maintaining proper time series validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cx_ppcpx0ngbpf4h354g9_k80000gn/T/ipykernel_43644/226080242.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-09-22 08:01:19.432168\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED IMPORTS\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for data_loader import\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from data_loader import load_data\n",
    "\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "pd.set_option(\"mode.chained_assignment\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/ret_sample.csv\n",
      "Data shape: (6401414, 159)\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/factor_char_list.csv\n",
      "Number of predictor variables: 147\n",
      "Data after removing missing returns: (6401414, 158)\n",
      "Date range: 2005-02-28 00:00:00 to 2025-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED DATA LOADING USING DATA HANDLER\n",
    "print(\"Loading data...\")\n",
    "raw = load_data(filename=\"ret_sample.csv\", parse_dates=[\"ret_eom\"], low_memory=False)\n",
    "print(f\"Data shape: {raw.shape}\")\n",
    "\n",
    "# Convert date once and work in place\n",
    "raw['date'] = pd.to_datetime(raw['ret_eom'])\n",
    "raw.drop('ret_eom', axis=1, inplace=True)  # Remove redundant column\n",
    "\n",
    "# Load predictor variables using data handler\n",
    "stock_vars = list(load_data(filename=\"factor_char_list.csv\")[\"variable\"].values)\n",
    "print(f\"Number of predictor variables: {len(stock_vars)}\")\n",
    "\n",
    "# Filter missing returns in place\n",
    "ret_var = \"stock_ret\"\n",
    "raw = raw[raw[ret_var].notna()].copy()  # Only copy once\n",
    "print(f\"Data after removing missing returns: {raw.shape}\")\n",
    "print(f\"Date range: {raw['date'].min()} to {raw['date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying rank transformations (vectorized)...\n",
      "Processed data shape: (6401414, 158)\n"
     ]
    }
   ],
   "source": [
    "# Vectorized rank transform that preserves 'date'\n",
    "print(\"Applying rank transformations (vectorized)...\")\n",
    "\n",
    "data = raw.copy()\n",
    "for var in stock_vars:\n",
    "    if var not in data.columns:\n",
    "        continue\n",
    "    med = data.groupby('date')[var].transform('median')\n",
    "    data[var] = data[var].fillna(med)\n",
    "\n",
    "    ranks = data.groupby('date')[var].rank(method='dense') - 1\n",
    "    maxs = data.groupby('date')[var].transform('max')\n",
    "    data[var] = np.where(maxs > 0, (ranks / maxs) * 2 - 1, 0)\n",
    "\n",
    "print(f\"Processed data shape: {data.shape}\")\n",
    "del raw  # free memory\n",
    "\n",
    "# Safety check\n",
    "assert 'date' in data.columns, \"date column missing after transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA functions defined\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED ARIMA FUNCTIONS\n",
    "def check_stationarity(series, title=''):\n",
    "    \"\"\"Optimized stationarity check\"\"\"\n",
    "    try:\n",
    "        result = adfuller(series.dropna())\n",
    "        return result[1] < 0.05\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def find_arima_params(series, max_p=2, max_d=1, max_q=2):  # Reduced search space\n",
    "    \"\"\"Optimized ARIMA parameter search\"\"\"\n",
    "    best_aic = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    # Reduced parameter space for speed\n",
    "    for p in range(max_p + 1):\n",
    "        for d in range(max_d + 1):\n",
    "            for q in range(max_q + 1):\n",
    "                try:\n",
    "                    model = ARIMA(series, order=(p, d, q))\n",
    "                    fitted_model = model.fit()\n",
    "                    if fitted_model.aic < best_aic:\n",
    "                        best_aic = fitted_model.aic\n",
    "                        best_params = (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "    return best_params, best_aic\n",
    "\n",
    "print(\"ARIMA functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available dates: 245 from 2005-02-28T00:00:00.000000000 to 2025-06-30T00:00:00.000000000\n",
      "\n",
      "Period 1: 2005-01-01 → 2016-01-01\n",
      "Train: 2321672, Val: 590345, Test: 300441\n",
      "\n",
      "Period 2: 2005-01-01 → 2017-01-01\n",
      "Train: 2615405, Val: 597053, Test: 304279\n"
     ]
    }
   ],
   "source": [
    "# Expanding-window ARIMA predictions that build pred_out\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Safety checks\n",
    "assert 'data' in globals(), \"Missing 'data' DataFrame. Run the data loading cells first.\"\n",
    "assert 'ret_var' in globals(), \"Missing ret_var. Run the data loading cell.\"\n",
    "\n",
    "pred_out = pd.DataFrame()  # <-- initialize here\n",
    "\n",
    "starting = pd.to_datetime(\"20050101\", format=\"%Y%m%d\")\n",
    "counter = 0\n",
    "\n",
    "unique_dates = np.sort(data['date'].unique())\n",
    "if len(unique_dates) == 0:\n",
    "    raise RuntimeError(\"No dates in 'data'. Check preprocessing.\")\n",
    "\n",
    "print(f\"Available dates: {len(unique_dates)} from {unique_dates[0]} to {unique_dates[-1]}\")\n",
    "\n",
    "while (starting + pd.DateOffset(years=11 + counter)) <= unique_dates[-1]:\n",
    "    cutoff = [\n",
    "        starting,\n",
    "        starting + pd.DateOffset(years=8 + counter),   # train\n",
    "        starting + pd.DateOffset(years=10 + counter),  # validate\n",
    "        starting + pd.DateOffset(years=11 + counter),  # test\n",
    "    ]\n",
    "    print(f\"\\nPeriod {counter+1}: {cutoff[0].date()} → {cutoff[3].date()}\")\n",
    "\n",
    "    train = data[(data[\"date\"] >= cutoff[0]) & (data[\"date\"] < cutoff[1])]\n",
    "    val   = data[(data[\"date\"] >= cutoff[1]) & (data[\"date\"] < cutoff[2])]\n",
    "    test  = data[(data[\"date\"] >= cutoff[2]) & (data[\"date\"] < cutoff[3])]\n",
    "\n",
    "    print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "    if len(train) == 0 or len(test) == 0:\n",
    "        print(\"Skipping (insufficient data).\")\n",
    "        counter += 1\n",
    "        continue\n",
    "\n",
    "    # Prepare output frame for this test window\n",
    "    reg_pred = test[[\"year\", \"month\", \"date\", \"id\", ret_var]].copy()\n",
    "    preds = np.full(len(test), np.nan)\n",
    "\n",
    "    # Predict per stock\n",
    "    test_stocks = test['id'].unique()\n",
    "    for stock_id in test_stocks:\n",
    "        stock_train = train[train['id'] == stock_id].sort_values('date')\n",
    "        stock_test_idx = test.index[test['id'] == stock_id]\n",
    "\n",
    "        if len(stock_train) < 10:\n",
    "            mean_ret = stock_train[ret_var].mean() if len(stock_train) else 0.0\n",
    "            preds[test.index.get_indexer(stock_test_idx)] = mean_ret\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ts = stock_train[ret_var].values\n",
    "            # Small param grid for speed; replace with your find_arima_params if defined\n",
    "            best, best_aic = None, np.inf\n",
    "            for p in (0,1,2):\n",
    "                for d in (0,1):\n",
    "                    for q in (0,1,2):\n",
    "                        try:\n",
    "                            fit = ARIMA(ts, order=(p,d,q)).fit()\n",
    "                            if fit.aic < best_aic:\n",
    "                                best, best_aic = (p,d,q), fit.aic\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            if best is None:\n",
    "                mean_ret = stock_train[ret_var].mean()\n",
    "                preds[test.index.get_indexer(stock_test_idx)] = mean_ret\n",
    "            else:\n",
    "                fit = ARIMA(ts, order=best).fit()\n",
    "                steps = len(stock_test_idx)\n",
    "                fc = fit.forecast(steps=steps)\n",
    "                if np.ndim(fc) == 0 or len(fc) == 1:\n",
    "                    fc = np.full(steps, float(fc[0] if len(np.atleast_1d(fc)) else 0.0))\n",
    "                preds[test.index.get_indexer(stock_test_idx)] = np.asarray(fc, dtype=float)\n",
    "        except:\n",
    "            mean_ret = stock_train[ret_var].mean()\n",
    "            preds[test.index.get_indexer(stock_test_idx)] = mean_ret\n",
    "\n",
    "    reg_pred[\"arima\"] = preds\n",
    "    pred_out = pd.concat([pred_out, reg_pred], ignore_index=True)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "print(f\"\\nCompleted periods: {counter}\")\n",
    "print(f\"pred_out shape: {pred_out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and calculate metrics\n",
    "print(\"Saving results...\")\n",
    "output_path = Path.cwd().parent / \"data\" / \"arima_predictions_v2.csv\"\n",
    "pred_out.to_csv(output_path, index=False)\n",
    "print(f\"Saved predictions to: {output_path}\")\n",
    "\n",
    "print(f\"\\nPrediction results shape: {pred_out.shape}\")\n",
    "print(f\"Non-null predictions: {pred_out['arima'].notna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display metrics\n",
    "yreal = pred_out[ret_var].values\n",
    "ypred = pred_out[\"arima\"].values\n",
    "\n",
    "# Remove NaN values\n",
    "mask = ~(np.isnan(yreal) | np.isnan(ypred))\n",
    "yreal_clean = yreal[mask]\n",
    "ypred_clean = ypred[mask]\n",
    "\n",
    "if len(yreal_clean) > 0:\n",
    "    r2 = 1 - np.sum(np.square((yreal_clean - ypred_clean))) / np.sum(np.square(yreal_clean))\n",
    "    mse = mean_squared_error(yreal_clean, ypred_clean)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(yreal_clean - ypred_clean))\n",
    "    \n",
    "    print(f\"\\n=== ARIMA v2 RESULTS ===\")\n",
    "    print(f\"R²: {r2:.6f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "    print(f\"MAE: {mae:.6f}\")\n",
    "    print(f\"Valid predictions: {len(yreal_clean)}\")\n",
    "    print(f\"Actual returns - Mean: {np.mean(yreal_clean):.6f}, Std: {np.std(yreal_clean):.6f}\")\n",
    "    print(f\"Predicted returns - Mean: {np.mean(ypred_clean):.6f}, Std: {np.std(ypred_clean):.6f}\")\n",
    "else:\n",
    "    print(\"ERROR: No valid predictions!\")\n",
    "\n",
    "print(f\"End time: {datetime.datetime.now()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZED EXPANDING WINDOW PREDICTION LOOP\n",
    "print(\"Starting ARIMA predictions with expanding window...\")\n",
    "\n",
    "# Initialize the starting date, counter, and output data\n",
    "starting = pd.to_datetime(\"20050101\", format=\"%Y%m%d\")\n",
    "counter = 0\n",
    "pred_out = pd.DataFrame()\n",
    "\n",
    "# Get unique dates for efficient filtering\n",
    "unique_dates = sorted(data['date'].unique())\n",
    "print(f\"Available dates: {len(unique_dates)} from {unique_dates[0]} to {unique_dates[-1]}\")\n",
    "\n",
    "# Estimation with expanding window - optimized version\n",
    "while (starting + pd.DateOffset(years=11 + counter)) <= unique_dates[-1]:\n",
    "    cutoff = [\n",
    "        starting,\n",
    "        starting + pd.DateOffset(years=8 + counter),  # Use 8 years and expanding as training set\n",
    "        starting + pd.DateOffset(years=10 + counter),  # Use next 2 years as validation set\n",
    "        starting + pd.DateOffset(years=11 + counter),\n",
    "    ]  # Use next year as out-of-sample testing set\n",
    "    \n",
    "    print(f\"\\nProcessing period {counter + 1}: {cutoff[0].strftime('%Y-%m-%d')} to {cutoff[3].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Efficient filtering using boolean indexing\n",
    "    train_mask = (data[\"date\"] >= cutoff[0]) & (data[\"date\"] < cutoff[1])\n",
    "    validate_mask = (data[\"date\"] >= cutoff[1]) & (data[\"date\"] < cutoff[2])\n",
    "    test_mask = (data[\"date\"] >= cutoff[2]) & (data[\"date\"] < cutoff[3])\n",
    "    \n",
    "    train_data = data[train_mask]\n",
    "    validate_data = data[validate_mask]\n",
    "    test_data = data[test_mask]\n",
    "    \n",
    "    print(f\"Training set: {len(train_data)} observations\")\n",
    "    print(f\"Validation set: {len(validate_data)} observations\")\n",
    "    print(f\"Test set: {len(test_data)} observations\")\n",
    "    \n",
    "    # Skip if any set is empty\n",
    "    if len(train_data) == 0 or len(test_data) == 0:\n",
    "        print(f\"Skipping period {counter + 1} - insufficient data\")\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    # Prepare output data\n",
    "    reg_pred = test_data[[\"year\", \"month\", \"date\", \"id\", ret_var]].copy()\n",
    "    \n",
    "    # Get unique stocks in test set\n",
    "    test_stocks = test_data['id'].unique()\n",
    "    print(f\"Predicting for {len(test_stocks)} stocks...\")\n",
    "    \n",
    "    # Pre-allocate predictions array\n",
    "    arima_predictions = np.full(len(test_data), np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Simple visualization\n",
    "if len(yreal_clean) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(yreal_clean, ypred_clean, alpha=0.5, s=1)\n",
    "    plt.plot([yreal_clean.min(), yreal_clean.max()], [yreal_clean.min(), yreal_clean.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Returns')\n",
    "    plt.ylabel('Predicted Returns')\n",
    "    plt.title('ARIMA v2: Predicted vs Actual Returns')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid predictions for visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Process stocks in batches for memory efficiency\n",
    "    batch_size = 1000\n",
    "    processed_stocks = 0\n",
    "    \n",
    "    for i in range(0, len(test_stocks), batch_size):\n",
    "        batch_stocks = test_stocks[i:i+batch_size]\n",
    "        \n",
    "        for stock_id in batch_stocks:\n",
    "            # Get data for this stock\n",
    "            stock_train = train_data[train_data['id'] == stock_id].sort_values('date')\n",
    "            stock_test_mask = test_data['id'] == stock_id\n",
    "            stock_test_indices = test_data[stock_test_mask].index\n",
    "            \n",
    "            if len(stock_train) < 10:\n",
    "                # Use mean prediction\n",
    "                mean_ret = stock_train[ret_var].mean() if len(stock_train) > 0 else 0\n",
    "                arima_predictions[test_data.index.get_indexer(stock_test_indices)] = mean_ret\n",
    "            else:\n",
    "                try:\n",
    "                    # Prepare time series\n",
    "                    train_series = stock_train[ret_var].values\n",
    "                    \n",
    "                    # Find optimal parameters (simplified)\n",
    "                    best_params, _ = find_arima_params(pd.Series(train_series))\n",
    "                    \n",
    "                    if best_params is not None:\n",
    "                        # Fit ARIMA model\n",
    "                        model = ARIMA(train_series, order=best_params)\n",
    "                        fitted_model = model.fit()\n",
    "                        \n",
    "                        # Make predictions\n",
    "                        n_predictions = len(stock_test_indices)\n",
    "                        arima_pred = fitted_model.forecast(steps=n_predictions)\n",
    "                        \n",
    "                        # Handle single prediction case\n",
    "                        if len(arima_pred) == 1:\n",
    "                            arima_pred = np.full(n_predictions, arima_pred[0])\n",
    "                        \n",
    "                        arima_predictions[test_data.index.get_indexer(stock_test_indices)] = arima_pred\n",
    "                    else:\n",
    "                        # Fallback to mean\n",
    "                        mean_ret = stock_train[ret_var].mean()\n",
    "                        arima_predictions[test_data.index.get_indexer(stock_test_indices)] = mean_ret\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Fallback to mean prediction\n",
    "                    mean_ret = stock_train[ret_var].mean()\n",
    "                    arima_predictions[test_data.index.get_indexer(stock_test_indices)] = mean_ret\n",
    "            \n",
    "            processed_stocks += 1\n",
    "            if processed_stocks % 100 == 0:\n",
    "                print(f\"  Processed {processed_stocks}/{len(test_stocks)} stocks\")\n",
    "    \n",
    "    # Add ARIMA predictions to output\n",
    "    reg_pred[\"arima\"] = arima_predictions\n",
    "    \n",
    "    # Add to the output data\n",
    "    pred_out = pd.concat([pred_out, reg_pred], ignore_index=True)\n",
    "    \n",
    "    # Go to the next year\n",
    "    counter += 1\n",
    "    \n",
    "    # Optional: Limit periods for testing (remove this for full run)\n",
    "    if counter >= 3:  # Limit to 3 periods for demonstration\n",
    "        print(\"Limiting to 3 periods for demonstration\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nCompleted processing {counter} periods\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
