{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "42831845-240f-4ec5-a088-a30d4755f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing and exploring the datasets for mapping DataDate-GVKEY-IID to company names\n",
    "# It creates 1943 duplicates because there are some that are duplicate for some reason......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a841d7b7-7cf2-46fb-998d-252128ebd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-01 01:16:43.445323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cx_ppcpx0ngbpf4h354g9_k80000gn/T/ipykernel_93529/2414979913.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Use shared data loader\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from data_loader import load_data\n",
    "\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "pd.set_option(\"mode.chained_assignment\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fc401fe5-37b3-4631-a0df-ebc1c52a93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_company_names_etc(raw, mappings):\n",
    "    mappings['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "    mappings['gvkey'] = mappings['gvkey'].astype(float)\n",
    "\n",
    "    raw['date'] = pd.to_datetime(raw['ret_eom'])\n",
    "\n",
    "    merged = pd.merge(raw, mappings3[['gvkey', 'datadate', 'iid', 'tic', 'conm', 'cusip','cik']], \n",
    "                    left_on=['gvkey', 'date', 'iid'], \n",
    "                    right_on=['gvkey', 'datadate', 'iid'], \n",
    "                    how='left')\n",
    "    \n",
    "    # checking duplicates for your info\n",
    "    # Check for duplicates in the merged DataFrame\n",
    "    original_row_count = raw.shape[0]\n",
    "    merged_row_count = merged.shape[0]\n",
    "\n",
    "    if original_row_count == merged_row_count:\n",
    "        print(\"No duplicates were created during the merge.\")\n",
    "    else:\n",
    "        print(f\"Duplicates detected: Original rows = {original_row_count}, Merged rows = {merged_row_count}, Difference = {merged_row_count - original_row_count}\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be2ff98d-9de6-4b8a-89a0-0d542f8aa123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/usa_can_filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "raw = load_data(filename=\"usa_can_filtered_data.csv\", parse_dates=[\"ret_eom\"], low_memory=False)\n",
    "raw['date'] = pd.to_datetime(raw['ret_eom'])\n",
    "# raw['iid'] = raw['iid'].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3be51a9b-4815-4c42-a5c8-edcfa3ff36c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/Global (ex Canada and US) Company Name Merge by DataDate-GVKEY-IID.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings = load_data(filename=\"Global (ex Canada and US) Company Name Merge by DataDate-GVKEY-IID.csv\", low_memory=False)\n",
    "mappings['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "mappings['gvkey'] = mappings['gvkey'].astype(float)\n",
    "# mappings['iid'] = mappings['iid'].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "179d691a-1169-4eb3-89ed-202cb63dd357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/cik_gvkey_linktable_USA_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings2 = load_data(filename=\"cik_gvkey_linktable_USA_only.csv\", low_memory=False)\n",
    "mappings2['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "mappings2['gvkey'] = mappings['gvkey'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3daac4de-f1c2-46aa-8c4d-9c5ef547b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/North America Company Name Merge by DataDate-GVKEY-IID.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings3 = load_data(filename=\"North America Company Name Merge by DataDate-GVKEY-IID.csv\", low_memory=False)\n",
    "mappings3['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "mappings3['gvkey'] = mappings['gvkey'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a3441bac-b563-4f38-830c-32c2228c876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates detected: Original rows = 1398807, Merged rows = 1400750, Difference = 1943\n"
     ]
    }
   ],
   "source": [
    "merged1 = merge_company_names_etc(raw, mappings3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8c1953e3-d6eb-45da-a91b-9c0a6a3bff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows based on 'gvkey', 'datadate', and 'iid': 970067\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>datadate</th>\n",
       "      <th>iid</th>\n",
       "      <th>tic</th>\n",
       "      <th>cusip</th>\n",
       "      <th>conm</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>1166.0</td>\n",
       "      <td>2006-03-31</td>\n",
       "      <td>01</td>\n",
       "      <td>BED</td>\n",
       "      <td>076446301</td>\n",
       "      <td>BEDFORD PROPERTY INVESTORS</td>\n",
       "      <td>910079.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>1166.0</td>\n",
       "      <td>2006-05-31</td>\n",
       "      <td>01</td>\n",
       "      <td>PHS</td>\n",
       "      <td>695112102</td>\n",
       "      <td>PACIFICARE HEALTH SYSTEMS</td>\n",
       "      <td>1027974.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1166.0</td>\n",
       "      <td>2006-06-30</td>\n",
       "      <td>01</td>\n",
       "      <td>DUCT</td>\n",
       "      <td>264175100</td>\n",
       "      <td>DUCT UTILITY CONSTR &amp; TECH</td>\n",
       "      <td>742876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>1166.0</td>\n",
       "      <td>2006-08-31</td>\n",
       "      <td>01</td>\n",
       "      <td>JAVA.1</td>\n",
       "      <td>866810203</td>\n",
       "      <td>SUN MICROSYSTEMS INC</td>\n",
       "      <td>709519.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>1166.0</td>\n",
       "      <td>2006-09-30</td>\n",
       "      <td>01</td>\n",
       "      <td>TROW</td>\n",
       "      <td>74144T108</td>\n",
       "      <td>PRICE (T. ROWE) GROUP</td>\n",
       "      <td>1113169.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438320</th>\n",
       "      <td>255898.0</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>90</td>\n",
       "      <td>LNSTY</td>\n",
       "      <td>54211N101</td>\n",
       "      <td>LONDON STOCK EXCH GROUP PLC</td>\n",
       "      <td>1842726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438323</th>\n",
       "      <td>255898.0</td>\n",
       "      <td>2024-09-30</td>\n",
       "      <td>90</td>\n",
       "      <td>MAUSY</td>\n",
       "      <td>576875207</td>\n",
       "      <td>MATSUI SECURITIES CO LTD</td>\n",
       "      <td>1197001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438325</th>\n",
       "      <td>255898.0</td>\n",
       "      <td>2024-11-30</td>\n",
       "      <td>90</td>\n",
       "      <td>TSYHY</td>\n",
       "      <td>89420Y209</td>\n",
       "      <td>TRAVELSKY TECHNOLOGY LTD</td>\n",
       "      <td>1200651.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438326</th>\n",
       "      <td>255898.0</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>01</td>\n",
       "      <td>PALAF</td>\n",
       "      <td>Q7264T252</td>\n",
       "      <td>PALADIN ENERGY LTD</td>\n",
       "      <td>1444331.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438328</th>\n",
       "      <td>255898.0</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>90</td>\n",
       "      <td>MFG</td>\n",
       "      <td>60687Y109</td>\n",
       "      <td>MIZUHO FINANCIAL GROUP INC</td>\n",
       "      <td>1335730.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>970067 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            gvkey   datadate iid     tic      cusip  \\\n",
       "2682       1166.0 2006-03-31  01     BED  076446301   \n",
       "2684       1166.0 2006-05-31  01     PHS  695112102   \n",
       "2685       1166.0 2006-06-30  01    DUCT  264175100   \n",
       "2687       1166.0 2006-08-31  01  JAVA.1  866810203   \n",
       "2688       1166.0 2006-09-30  01    TROW  74144T108   \n",
       "...           ...        ...  ..     ...        ...   \n",
       "4438320  255898.0 2024-06-30  90   LNSTY  54211N101   \n",
       "4438323  255898.0 2024-09-30  90   MAUSY  576875207   \n",
       "4438325  255898.0 2024-11-30  90   TSYHY  89420Y209   \n",
       "4438326  255898.0 2024-12-31  01   PALAF  Q7264T252   \n",
       "4438328  255898.0 2025-02-28  90     MFG  60687Y109   \n",
       "\n",
       "                                conm        cik  \n",
       "2682      BEDFORD PROPERTY INVESTORS   910079.0  \n",
       "2684       PACIFICARE HEALTH SYSTEMS  1027974.0  \n",
       "2685      DUCT UTILITY CONSTR & TECH   742876.0  \n",
       "2687            SUN MICROSYSTEMS INC   709519.0  \n",
       "2688           PRICE (T. ROWE) GROUP  1113169.0  \n",
       "...                              ...        ...  \n",
       "4438320  LONDON STOCK EXCH GROUP PLC  1842726.0  \n",
       "4438323     MATSUI SECURITIES CO LTD  1197001.0  \n",
       "4438325     TRAVELSKY TECHNOLOGY LTD  1200651.0  \n",
       "4438326           PALADIN ENERGY LTD  1444331.0  \n",
       "4438328   MIZUHO FINANCIAL GROUP INC  1335730.0  \n",
       "\n",
       "[970067 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values in 'gvkey', 'datadate', or 'iid'\n",
    "missing_values = mappings3[mappings3[['gvkey', 'datadate', 'iid']].isna().any(axis=1)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"Rows with missing values in 'gvkey', 'datadate', or 'iid': {len(missing_values)}\")\n",
    "    display(missing_values)  # Use display() in a notebook to view the rows\n",
    "\n",
    "# Check if 'gvkey', 'datadate', and 'iid' are unique\n",
    "duplicates = mappings3[mappings3.duplicated(subset=['gvkey', 'datadate', 'iid'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(f\"Duplicate rows based on 'gvkey', 'datadate', and 'iid': {len(duplicates)}\")\n",
    "    display(duplicates)  # Use display() in a notebook to view the rows\n",
    "else:\n",
    "    print(\"No duplicates found based on 'gvkey', 'datadate', and 'iid'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f3a5c508-bbe8-48d3-b520-d61ad1ac84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example duplicate rows in mappings3:\n",
      "       gvkey   datadate iid    tic      cusip                        conm  \\\n",
      "2682  1166.0 2006-03-31  01    BED  076446301  BEDFORD PROPERTY INVESTORS   \n",
      "2920  1166.0 2006-03-31  01  AXIHQ  05462D101     AXION INTL HOLDINGS INC   \n",
      "\n",
      "           cik  \n",
      "2682  910079.0  \n",
      "2920  753048.0  \n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in mappings3\n",
    "if not is_unique:\n",
    "    # Get one example of a duplicate group\n",
    "    example_duplicate_key = duplicate_rows[['gvkey', 'datadate', 'iid']].iloc[0]\n",
    "    gvkey_example = example_duplicate_key['gvkey']\n",
    "    datadate_example = example_duplicate_key['datadate']\n",
    "    iid_example = example_duplicate_key['iid']\n",
    "\n",
    "    # Filter rows for this specific duplicate group\n",
    "    example_rows = mappings3[\n",
    "        (mappings3['gvkey'] == gvkey_example) &\n",
    "        (mappings3['datadate'] == datadate_example) &\n",
    "        (mappings3['iid'] == iid_example)\n",
    "    ]\n",
    "\n",
    "    print(\"Example duplicate rows in mappings3:\")\n",
    "    print(example_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4edcf0fe-e90d-47ae-a01e-581ed854d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mappings onto raw\n",
    "merged = pd.merge(raw, mappings3[['gvkey', 'datadate', 'iid', 'tic', 'conm', 'cusip','cik']], \n",
    "                  left_on=['gvkey', 'date', 'iid'], \n",
    "                  right_on=['gvkey', 'datadate', 'iid'], \n",
    "                  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e33e6aec-06f1-4072-b71e-73aca685d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates detected: Original rows = 1398807, Merged rows = 1400750, Difference = 1943\n",
      "Duplicate rows:\n",
      "                     id       date    ret_eom     gvkey  iid excntry  \\\n",
      "2147         crsp_19227 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "5830         crsp_88258 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "6131         crsp_89139 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "9008         crsp_19227 2005-03-31 2005-03-31       NaN  NaN     USA   \n",
      "12674        crsp_88258 2005-03-31 2005-03-31       NaN  NaN     USA   \n",
      "...                 ...        ...        ...       ...  ...     ...   \n",
      "1399646  comp_063186_01 2025-06-30 2025-06-30   63186.0   01     USA   \n",
      "1399800  comp_101276_01 2025-06-30 2025-06-30  101276.0   01     USA   \n",
      "1399801  comp_101276_01 2025-06-30 2025-06-30  101276.0   01     USA   \n",
      "1399802  comp_101973_01 2025-06-30 2025-06-30  101973.0   01     USA   \n",
      "1399803  comp_101973_01 2025-06-30 2025-06-30  101973.0   01     USA   \n",
      "\n",
      "         stock_ret  year  month  char_date  ...  age       qmj  qmj_prof  \\\n",
      "2147     -0.111594  2005      2   20050131  ...  124       NaN       NaN   \n",
      "5830     -0.090909  2005      2   20050131  ...  265       NaN       NaN   \n",
      "6131      0.269888  2005      2   20050131  ...   42       NaN       NaN   \n",
      "9008     -0.385107  2005      3   20050228  ...  125       NaN       NaN   \n",
      "12674     0.076000  2005      3   20050228  ...  266       NaN       NaN   \n",
      "...            ...   ...    ...        ...  ...  ...       ...       ...   \n",
      "1399646   0.064924  2025      6   20250530  ...  497  1.281307  0.944952   \n",
      "1399800  -0.010848  2025      6   20250530  ...  497 -1.392927 -1.203586   \n",
      "1399801  -0.010848  2025      6   20250530  ...  497 -1.392927 -1.203586   \n",
      "1399802   0.042553  2025      6   20250530  ...  497  1.582061  1.602373   \n",
      "1399803   0.042553  2025      6   20250530  ...  497  1.582061  1.602373   \n",
      "\n",
      "         qmj_growth  qmj_safety   datadate     tic                       conm  \\\n",
      "2147            NaN   -1.020464        NaT     NaN                        NaN   \n",
      "5830            NaN   -1.606884        NaT     NaN                        NaN   \n",
      "6131            NaN   -1.355561        NaT     NaN                        NaN   \n",
      "9008            NaN   -1.096431        NaT     NaN                        NaN   \n",
      "12674           NaN   -1.604002        NaT     NaN                        NaN   \n",
      "...             ...         ...        ...     ...                        ...   \n",
      "1399646    0.797619    0.626433 2025-06-30    TMTP           TMT CAPITAL CORP   \n",
      "1399800   -1.489044    0.001440 2025-06-30    ELTP  ELITE PHARMACEUTICALS INC   \n",
      "1399801   -1.489044    0.001440 2025-06-30    EXCH    EXCHANGE BANKSHARES INC   \n",
      "1399802    1.650274    0.273614 2025-06-30    CBRF  CYBERFUELS HOLDING CO INC   \n",
      "1399803    1.650274    0.273614 2025-06-30  ELON.1               ECHELON CORP   \n",
      "\n",
      "             cusip        cik  \n",
      "2147           NaN        NaN  \n",
      "5830           NaN        NaN  \n",
      "6131           NaN        NaN  \n",
      "9008           NaN        NaN  \n",
      "12674          NaN        NaN  \n",
      "...            ...        ...  \n",
      "1399646  872589106        NaN  \n",
      "1399800  28659T200  1053369.0  \n",
      "1399801  301258109   725618.0  \n",
      "1399802  278744107  1116618.0  \n",
      "1399803  27874N303    31347.0  \n",
      "\n",
      "[3747 rows x 164 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the merged DataFrame\n",
    "original_row_count = raw.shape[0]\n",
    "merged_row_count = merged.shape[0]\n",
    "\n",
    "if original_row_count == merged_row_count:\n",
    "    print(\"No duplicates were created during the merge.\")\n",
    "else:\n",
    "    print(f\"Duplicates detected: Original rows = {original_row_count}, Merged rows = {merged_row_count}, Difference = {merged_row_count - original_row_count}\")\n",
    "\n",
    "# Optional: Inspect rows if duplicates are detected\n",
    "if original_row_count != merged_row_count:\n",
    "    duplicate_rows = merged[merged.duplicated(subset=['gvkey', 'date', 'iid'], keep=False)]\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ee2dac6-2cc1-4369-a46e-676f094cbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping iids: {'11', '07', '03', '05', '06', '02', '05C', '90', '04', '01C', '12', '07C', '19', '04C', '02C', '03C', '08', '01', '09', '10', '21'}\n"
     ]
    }
   ],
   "source": [
    "overlapping_iids = set(raw['iid']).intersection(set(mappings3['iid']))\n",
    "print(\"Overlapping iids:\", overlapping_iids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04b8f48b-f148-4e63-9ce3-16edf7ed06cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01W\n",
       "1    01W\n",
       "2    01W\n",
       "3    01W\n",
       "4    01W\n",
       "Name: iid, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings['iid'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f192594-e27d-4321-afa2-5bfb2b11e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique iids in raw:\n",
      "['01C' '02' '04C' '02C' '01' '03C' '03' '05C' '90' nan '05' '04' '08' '06'\n",
      " '09' '10' '07' '07C' '19' '11' '12' '21']\n",
      "Unique iids in mappings:\n",
      "['01' '02' '01C' '10' '90' '03' '17' '24' '04C' '02C' '15' '91' '06' '04'\n",
      " '90C' '05' '03C' '07' '08' '09' '12' '13' '14' '44' '38' '26' '31' '93'\n",
      " '06C' '22' '23' '25' '27' '28' '29' '30' '34' '36' '37' '54' '57' '76'\n",
      " '92' '11' '16' '18' '19' '08C' '82' '89' '94' '95' '97' '99' '05C' '96'\n",
      " '86' '98' '47' '39' '75' '81' '09C' '40' '88' '41' '62' '35' '20' '71'\n",
      " '07C' '39C' '11C' '74' '87' '85' '43' '10C' '32' '84' '21' '12C' '13C'\n",
      " '33' '14C' '42' '15C' '16C' '17C' '18C' '19C' '20C' '21C' '22C' '23C'\n",
      " '24C' '25C' '26C' '27C' '28C' '64' '80' '75C' '49' '29C' '36C' '32C'\n",
      " '41C' '37C' '33C' '69' '67' '31C' '64C' '62C' '63C' '66C' '42C' '30C']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique iids in raw:\")\n",
    "print(raw['iid'].unique())\n",
    "\n",
    "print(\"Unique iids in mappings:\")\n",
    "print(mappings2['iid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "237656ac-cc3f-47b1-962d-2219dbf79473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups with duplicates: 0\n",
      "Number of groups where all duplicates are identical: 0\n",
      "Number of groups where duplicates are not identical: 0\n",
      "Non-identical duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [id, date, ret_eom, gvkey, iid, excntry, stock_ret, year, month, char_date, char_eom, me, prc, market_equity, div12m_me, chcsho_12m, eqnpo_12m, ret_1_0, ret_3_1, ret_6_1, ret_9_1, ret_12_1, ret_12_7, ret_60_12, seas_1_1an, seas_1_1na, seas_2_5an, seas_2_5na, at_gr1, sale_gr1, capx_gr1, inv_gr1, debt_gr3, sale_gr3, capx_gr3, inv_gr1a, lti_gr1a, sti_gr1a, coa_gr1a, col_gr1a, cowc_gr1a, ncoa_gr1a, ncol_gr1a, nncoa_gr1a, fnl_gr1a, nfna_gr1a, tax_gr1a, be_gr1a, ebit_sale, gp_at, cop_at, ope_be, ni_be, ebit_bev, netis_at, eqnetis_at, dbnetis_at, oaccruals_at, oaccruals_ni, taccruals_at, taccruals_ni, noa_at, opex_at, at_turnover, sale_bev, rd_sale, cash_at, sale_emp_gr1, emp_gr1, ni_inc8q, noa_gr1a, ppeinv_gr1a, lnoa_gr1a, capx_gr2, saleq_gr1, niq_be, niq_at, niq_be_chg1, niq_at_chg1, rd5_at, dsale_dinv, dsale_drec, dgp_dsale, dsale_dsga, saleq_su, niq_su, capex_abn, op_atl1, gp_atl1, ope_bel1, cop_atl1, pi_nix, ocf_at, op_at, ocf_at_chg1, at_be, ocfq_saleq_std, tangibility, earnings_variability, aliq_at, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 159 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cx_ppcpx0ngbpf4h354g9_k80000gn/T/ipykernel_93529/209578681.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  identical_groups = duplicate_groups.groupby(['gvkey', 'date', 'iid']).apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by the specified columns\n",
    "grouped = raw.groupby(['gvkey', 'date', 'iid'])\n",
    "\n",
    "# Find groups with duplicates\n",
    "duplicate_groups = grouped.filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Check if all rows within each group are identical\n",
    "identical_groups = duplicate_groups.groupby(['gvkey', 'date', 'iid']).apply(\n",
    "    lambda group: group.nunique().eq(1).all()\n",
    ")\n",
    "\n",
    "# Separate identical and non-identical groups\n",
    "all_identical = identical_groups[identical_groups].index\n",
    "not_identical = identical_groups[~identical_groups].index\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of groups with duplicates: {len(identical_groups)}\")\n",
    "print(f\"Number of groups where all duplicates are identical: {len(all_identical)}\")\n",
    "print(f\"Number of groups where duplicates are not identical: {len(not_identical)}\")\n",
    "\n",
    "# Optionally, inspect non-identical groups\n",
    "non_identical_rows = duplicate_groups[\n",
    "    duplicate_groups.set_index(['gvkey', 'date', 'iid']).index.isin(not_identical)\n",
    "]\n",
    "print(\"Non-identical duplicate rows:\")\n",
    "print(non_identical_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
