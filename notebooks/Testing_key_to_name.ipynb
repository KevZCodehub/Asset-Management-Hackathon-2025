{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42831845-240f-4ec5-a088-a30d4755f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing and exploring the datasets for mapping DataDate-GVKEY-IID to company names\n",
    "# It creates 1943 duplicates because there are some that are duplicate for some reason......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a841d7b7-7cf2-46fb-998d-252128ebd6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2025-10-01 12:16:07.409315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cx_ppcpx0ngbpf4h354g9_k80000gn/T/ipykernel_4741/2414979913.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Use shared data loader\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "from data_loader import load_data\n",
    "\n",
    "print(f\"Start time: {datetime.datetime.now()}\")\n",
    "pd.set_option(\"mode.chained_assignment\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc401fe5-37b3-4631-a0df-ebc1c52a93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_company_names_etc(raw, mappings):\n",
    "    mappings['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "\n",
    "    raw['date'] = pd.to_datetime(raw['ret_eom'])\n",
    "\n",
    "    merged = pd.merge(raw, mappings[['gvkey', 'datadate', 'iid', 'tic', 'conm', 'cusip','cik']], \n",
    "                    left_on=['gvkey', 'date', 'iid'], \n",
    "                    right_on=['gvkey', 'datadate', 'iid'], \n",
    "                    how='left')\n",
    "    \n",
    "    # checking duplicates for your info\n",
    "    # Check for duplicates in the merged DataFrame\n",
    "    original_row_count = raw.shape[0]\n",
    "    merged_row_count = merged.shape[0]\n",
    "\n",
    "    if original_row_count == merged_row_count:\n",
    "        print(\"No duplicates were created during the merge.\")\n",
    "    else:\n",
    "        print(f\"Duplicates detected: Original rows = {original_row_count}, Merged rows = {merged_row_count}, Difference = {merged_row_count - original_row_count}\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2ff98d-9de6-4b8a-89a0-0d542f8aa123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/usa_can_filtered_data.csv\n"
     ]
    }
   ],
   "source": [
    "raw = load_data(filename=\"usa_can_filtered_data.csv\", parse_dates=[\"ret_eom\"], low_memory=False)\n",
    "raw['date'] = pd.to_datetime(raw['ret_eom'])\n",
    "# raw['iid'] = raw['iid'].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3be51a9b-4815-4c42-a5c8-edcfa3ff36c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/Global (ex Canada and US) Company Name Merge by DataDate-GVKEY-IID.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings = load_data(filename=\"Global (ex Canada and US) Company Name Merge by DataDate-GVKEY-IID.csv\", low_memory=False)\n",
    "mappings['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "mappings['gvkey'] = mappings['gvkey'].astype(float)\n",
    "# mappings['iid'] = mappings['iid'].astype(str).str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "179d691a-1169-4eb3-89ed-202cb63dd357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/cik_gvkey_linktable_USA_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings2 = load_data(filename=\"cik_gvkey_linktable_USA_only.csv\", low_memory=False)\n",
    "mappings2['datadate'] = pd.to_datetime(mappings['datadate'])\n",
    "mappings2['gvkey'] = mappings['gvkey'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daac4de-f1c2-46aa-8c4d-9c5ef547b0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data from: /Users/kevin/Coding Projects/Asset-Management-Hackathon-2025/data/North America Company Name Merge by DataDate-GVKEY-IID.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data using the shared data loader\n",
    "print(\"Loading data...\")\n",
    "\n",
    "mappings3 = load_data(filename=\"North America Company Name Merge by DataDate-GVKEY-IID.csv\", low_memory=False)\n",
    "mappings3['datadate'] = pd.to_datetime(mappings3['datadate'])\n",
    "mappings3['gvkey'] = mappings3['gvkey'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3441bac-b563-4f38-830c-32c2228c876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates were created during the merge.\n"
     ]
    }
   ],
   "source": [
    "merged1 = merge_company_names_etc(raw, mappings3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c1953e3-d6eb-45da-a91b-9c0a6a3bff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found based on 'gvkey', 'datadate', and 'iid'.\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in 'gvkey', 'datadate', or 'iid'\n",
    "missing_values = mappings3[mappings3[['gvkey', 'datadate', 'iid']].isna().any(axis=1)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"Rows with missing values in 'gvkey', 'datadate', or 'iid': {len(missing_values)}\")\n",
    "    display(missing_values)  # Use display() in a notebook to view the rows\n",
    "\n",
    "# Check if 'gvkey', 'datadate', and 'iid' are unique\n",
    "duplicates = mappings3[mappings3.duplicated(subset=['gvkey', 'datadate', 'iid'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(f\"Duplicate rows based on 'gvkey', 'datadate', and 'iid': {len(duplicates)}\")\n",
    "    display(duplicates)  # Use display() in a notebook to view the rows\n",
    "else:\n",
    "    print(\"No duplicates found based on 'gvkey', 'datadate', and 'iid'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a5c508-bbe8-48d3-b520-d61ad1ac84a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check for duplicates in mappings3\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_unique\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Get one example of a duplicate group\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     example_duplicate_key \u001b[38;5;241m=\u001b[39m duplicate_rows[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgvkey\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatadate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miid\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     gvkey_example \u001b[38;5;241m=\u001b[39m example_duplicate_key[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgvkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_unique' is not defined"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in mappings3\n",
    "if not is_unique:\n",
    "    # Get one example of a duplicate group\n",
    "    example_duplicate_key = duplicate_rows[['gvkey', 'datadate', 'iid']].iloc[0]\n",
    "    gvkey_example = example_duplicate_key['gvkey']\n",
    "    datadate_example = example_duplicate_key['datadate']\n",
    "    iid_example = example_duplicate_key['iid']\n",
    "\n",
    "    # Filter rows for this specific duplicate group\n",
    "    example_rows = mappings3[\n",
    "        (mappings3['gvkey'] == gvkey_example) &\n",
    "        (mappings3['datadate'] == datadate_example) &\n",
    "        (mappings3['iid'] == iid_example)\n",
    "    ]\n",
    "\n",
    "    print(\"Example duplicate rows in mappings3:\")\n",
    "    print(example_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4edcf0fe-e90d-47ae-a01e-581ed854d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge mappings onto raw\n",
    "merged = pd.merge(raw, mappings3[['gvkey', 'datadate', 'iid', 'tic', 'conm', 'cusip','cik']], \n",
    "                  left_on=['gvkey', 'date', 'iid'], \n",
    "                  right_on=['gvkey', 'datadate', 'iid'], \n",
    "                  how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e33e6aec-06f1-4072-b71e-73aca685d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates detected: Original rows = 1398807, Merged rows = 1400750, Difference = 1943\n",
      "Duplicate rows:\n",
      "                     id       date    ret_eom     gvkey  iid excntry  \\\n",
      "2147         crsp_19227 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "5830         crsp_88258 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "6131         crsp_89139 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
      "9008         crsp_19227 2005-03-31 2005-03-31       NaN  NaN     USA   \n",
      "12674        crsp_88258 2005-03-31 2005-03-31       NaN  NaN     USA   \n",
      "...                 ...        ...        ...       ...  ...     ...   \n",
      "1399646  comp_063186_01 2025-06-30 2025-06-30   63186.0   01     USA   \n",
      "1399800  comp_101276_01 2025-06-30 2025-06-30  101276.0   01     USA   \n",
      "1399801  comp_101276_01 2025-06-30 2025-06-30  101276.0   01     USA   \n",
      "1399802  comp_101973_01 2025-06-30 2025-06-30  101973.0   01     USA   \n",
      "1399803  comp_101973_01 2025-06-30 2025-06-30  101973.0   01     USA   \n",
      "\n",
      "         stock_ret  year  month  char_date  ...  age       qmj  qmj_prof  \\\n",
      "2147     -0.111594  2005      2   20050131  ...  124       NaN       NaN   \n",
      "5830     -0.090909  2005      2   20050131  ...  265       NaN       NaN   \n",
      "6131      0.269888  2005      2   20050131  ...   42       NaN       NaN   \n",
      "9008     -0.385107  2005      3   20050228  ...  125       NaN       NaN   \n",
      "12674     0.076000  2005      3   20050228  ...  266       NaN       NaN   \n",
      "...            ...   ...    ...        ...  ...  ...       ...       ...   \n",
      "1399646   0.064924  2025      6   20250530  ...  497  1.281307  0.944952   \n",
      "1399800  -0.010848  2025      6   20250530  ...  497 -1.392927 -1.203586   \n",
      "1399801  -0.010848  2025      6   20250530  ...  497 -1.392927 -1.203586   \n",
      "1399802   0.042553  2025      6   20250530  ...  497  1.582061  1.602373   \n",
      "1399803   0.042553  2025      6   20250530  ...  497  1.582061  1.602373   \n",
      "\n",
      "         qmj_growth  qmj_safety   datadate     tic                       conm  \\\n",
      "2147            NaN   -1.020464        NaT     NaN                        NaN   \n",
      "5830            NaN   -1.606884        NaT     NaN                        NaN   \n",
      "6131            NaN   -1.355561        NaT     NaN                        NaN   \n",
      "9008            NaN   -1.096431        NaT     NaN                        NaN   \n",
      "12674           NaN   -1.604002        NaT     NaN                        NaN   \n",
      "...             ...         ...        ...     ...                        ...   \n",
      "1399646    0.797619    0.626433 2025-06-30    TMTP           TMT CAPITAL CORP   \n",
      "1399800   -1.489044    0.001440 2025-06-30    ELTP  ELITE PHARMACEUTICALS INC   \n",
      "1399801   -1.489044    0.001440 2025-06-30    EXCH    EXCHANGE BANKSHARES INC   \n",
      "1399802    1.650274    0.273614 2025-06-30    CBRF  CYBERFUELS HOLDING CO INC   \n",
      "1399803    1.650274    0.273614 2025-06-30  ELON.1               ECHELON CORP   \n",
      "\n",
      "             cusip        cik  \n",
      "2147           NaN        NaN  \n",
      "5830           NaN        NaN  \n",
      "6131           NaN        NaN  \n",
      "9008           NaN        NaN  \n",
      "12674          NaN        NaN  \n",
      "...            ...        ...  \n",
      "1399646  872589106        NaN  \n",
      "1399800  28659T200  1053369.0  \n",
      "1399801  301258109   725618.0  \n",
      "1399802  278744107  1116618.0  \n",
      "1399803  27874N303    31347.0  \n",
      "\n",
      "[3747 rows x 164 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the merged DataFrame\n",
    "original_row_count = raw.shape[0]\n",
    "merged_row_count = merged.shape[0]\n",
    "\n",
    "if original_row_count == merged_row_count:\n",
    "    print(\"No duplicates were created during the merge.\")\n",
    "else:\n",
    "    print(f\"Duplicates detected: Original rows = {original_row_count}, Merged rows = {merged_row_count}, Difference = {merged_row_count - original_row_count}\")\n",
    "\n",
    "# Optional: Inspect rows if duplicates are detected\n",
    "if original_row_count != merged_row_count:\n",
    "    duplicate_rows = merged[merged.duplicated(subset=['gvkey', 'date', 'iid'], keep=False)]\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ee2dac6-2cc1-4369-a46e-676f094cbeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping iids: {'11', '07', '03', '05', '06', '02', '05C', '90', '04', '01C', '12', '07C', '19', '04C', '02C', '03C', '08', '01', '09', '10', '21'}\n"
     ]
    }
   ],
   "source": [
    "overlapping_iids = set(raw['iid']).intersection(set(mappings3['iid']))\n",
    "print(\"Overlapping iids:\", overlapping_iids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "04b8f48b-f148-4e63-9ce3-16edf7ed06cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    01W\n",
       "1    01W\n",
       "2    01W\n",
       "3    01W\n",
       "4    01W\n",
       "Name: iid, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings['iid'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2f192594-e27d-4321-afa2-5bfb2b11e566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique iids in raw:\n",
      "['01C' '02' '04C' '02C' '01' '03C' '03' '05C' '90' nan '05' '04' '08' '06'\n",
      " '09' '10' '07' '07C' '19' '11' '12' '21']\n",
      "Unique iids in mappings:\n",
      "['01' '02' '01C' '10' '90' '03' '17' '24' '04C' '02C' '15' '91' '06' '04'\n",
      " '90C' '05' '03C' '07' '08' '09' '12' '13' '14' '44' '38' '26' '31' '93'\n",
      " '06C' '22' '23' '25' '27' '28' '29' '30' '34' '36' '37' '54' '57' '76'\n",
      " '92' '11' '16' '18' '19' '08C' '82' '89' '94' '95' '97' '99' '05C' '96'\n",
      " '86' '98' '47' '39' '75' '81' '09C' '40' '88' '41' '62' '35' '20' '71'\n",
      " '07C' '39C' '11C' '74' '87' '85' '43' '10C' '32' '84' '21' '12C' '13C'\n",
      " '33' '14C' '42' '15C' '16C' '17C' '18C' '19C' '20C' '21C' '22C' '23C'\n",
      " '24C' '25C' '26C' '27C' '28C' '64' '80' '75C' '49' '29C' '36C' '32C'\n",
      " '41C' '37C' '33C' '69' '67' '31C' '64C' '62C' '63C' '66C' '42C' '30C']\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique iids in raw:\")\n",
    "print(raw['iid'].unique())\n",
    "\n",
    "print(\"Unique iids in mappings:\")\n",
    "print(mappings2['iid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "237656ac-cc3f-47b1-962d-2219dbf79473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups with duplicates: 0\n",
      "Number of groups where all duplicates are identical: 0\n",
      "Number of groups where duplicates are not identical: 0\n",
      "Non-identical duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [id, date, ret_eom, gvkey, iid, excntry, stock_ret, year, month, char_date, char_eom, me, prc, market_equity, div12m_me, chcsho_12m, eqnpo_12m, ret_1_0, ret_3_1, ret_6_1, ret_9_1, ret_12_1, ret_12_7, ret_60_12, seas_1_1an, seas_1_1na, seas_2_5an, seas_2_5na, at_gr1, sale_gr1, capx_gr1, inv_gr1, debt_gr3, sale_gr3, capx_gr3, inv_gr1a, lti_gr1a, sti_gr1a, coa_gr1a, col_gr1a, cowc_gr1a, ncoa_gr1a, ncol_gr1a, nncoa_gr1a, fnl_gr1a, nfna_gr1a, tax_gr1a, be_gr1a, ebit_sale, gp_at, cop_at, ope_be, ni_be, ebit_bev, netis_at, eqnetis_at, dbnetis_at, oaccruals_at, oaccruals_ni, taccruals_at, taccruals_ni, noa_at, opex_at, at_turnover, sale_bev, rd_sale, cash_at, sale_emp_gr1, emp_gr1, ni_inc8q, noa_gr1a, ppeinv_gr1a, lnoa_gr1a, capx_gr2, saleq_gr1, niq_be, niq_at, niq_be_chg1, niq_at_chg1, rd5_at, dsale_dinv, dsale_drec, dgp_dsale, dsale_dsga, saleq_su, niq_su, capex_abn, op_atl1, gp_atl1, ope_bel1, cop_atl1, pi_nix, ocf_at, op_at, ocf_at_chg1, at_be, ocfq_saleq_std, tangibility, earnings_variability, aliq_at, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 159 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cx_ppcpx0ngbpf4h354g9_k80000gn/T/ipykernel_93529/209578681.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  identical_groups = duplicate_groups.groupby(['gvkey', 'date', 'iid']).apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by the specified columns\n",
    "grouped = raw.groupby(['gvkey', 'date', 'iid'])\n",
    "\n",
    "# Find groups with duplicates\n",
    "duplicate_groups = grouped.filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Check if all rows within each group are identical\n",
    "identical_groups = duplicate_groups.groupby(['gvkey', 'date', 'iid']).apply(\n",
    "    lambda group: group.nunique().eq(1).all()\n",
    ")\n",
    "\n",
    "# Separate identical and non-identical groups\n",
    "all_identical = identical_groups[identical_groups].index\n",
    "not_identical = identical_groups[~identical_groups].index\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of groups with duplicates: {len(identical_groups)}\")\n",
    "print(f\"Number of groups where all duplicates are identical: {len(all_identical)}\")\n",
    "print(f\"Number of groups where duplicates are not identical: {len(not_identical)}\")\n",
    "\n",
    "# Optionally, inspect non-identical groups\n",
    "non_identical_rows = duplicate_groups[\n",
    "    duplicate_groups.set_index(['gvkey', 'date', 'iid']).index.isin(not_identical)\n",
    "]\n",
    "print(\"Non-identical duplicate rows:\")\n",
    "print(non_identical_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cf62a6a-56bd-428f-bee3-c3df97dd57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN for all merged columns: 935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>ret_eom</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>iid</th>\n",
       "      <th>excntry</th>\n",
       "      <th>stock_ret</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>char_date</th>\n",
       "      <th>...</th>\n",
       "      <th>age</th>\n",
       "      <th>qmj</th>\n",
       "      <th>qmj_prof</th>\n",
       "      <th>qmj_growth</th>\n",
       "      <th>qmj_safety</th>\n",
       "      <th>datadate</th>\n",
       "      <th>tic</th>\n",
       "      <th>conm</th>\n",
       "      <th>cusip</th>\n",
       "      <th>cik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>comp_144498_01C</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>144498.0</td>\n",
       "      <td>01C</td>\n",
       "      <td>CAN</td>\n",
       "      <td>-0.395696</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>20050131</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>crsp_19227</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.111594</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>20050131</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.020464</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3145</th>\n",
       "      <td>crsp_71985</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.050633</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>20050131</td>\n",
       "      <td>...</td>\n",
       "      <td>265</td>\n",
       "      <td>0.438271</td>\n",
       "      <td>-0.346311</td>\n",
       "      <td>0.074514</td>\n",
       "      <td>1.153051</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>crsp_88258</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>20050131</td>\n",
       "      <td>...</td>\n",
       "      <td>265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.606884</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>crsp_89139</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>2005-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.269888</td>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>20050131</td>\n",
       "      <td>...</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.355561</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383497</th>\n",
       "      <td>comp_009919_01</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>2025-02-28</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.161589</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>20250131</td>\n",
       "      <td>...</td>\n",
       "      <td>505</td>\n",
       "      <td>-0.960044</td>\n",
       "      <td>-0.637429</td>\n",
       "      <td>0.172076</td>\n",
       "      <td>-1.030098</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386690</th>\n",
       "      <td>comp_009919_01</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>-0.192615</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>20250228</td>\n",
       "      <td>...</td>\n",
       "      <td>506</td>\n",
       "      <td>-0.937657</td>\n",
       "      <td>-0.629447</td>\n",
       "      <td>0.194402</td>\n",
       "      <td>-1.015891</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389872</th>\n",
       "      <td>comp_009919_01</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.045735</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>20250331</td>\n",
       "      <td>...</td>\n",
       "      <td>507</td>\n",
       "      <td>-0.947467</td>\n",
       "      <td>-0.639770</td>\n",
       "      <td>0.193889</td>\n",
       "      <td>-1.014075</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393041</th>\n",
       "      <td>comp_009919_01</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.246311</td>\n",
       "      <td>2025</td>\n",
       "      <td>5</td>\n",
       "      <td>20250430</td>\n",
       "      <td>...</td>\n",
       "      <td>508</td>\n",
       "      <td>-1.181943</td>\n",
       "      <td>-0.537743</td>\n",
       "      <td>-0.500379</td>\n",
       "      <td>-0.944170</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396196</th>\n",
       "      <td>comp_009919_01</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>2025-06-30</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>01</td>\n",
       "      <td>USA</td>\n",
       "      <td>0.195424</td>\n",
       "      <td>2025</td>\n",
       "      <td>6</td>\n",
       "      <td>20250530</td>\n",
       "      <td>...</td>\n",
       "      <td>509</td>\n",
       "      <td>-1.177438</td>\n",
       "      <td>-0.538941</td>\n",
       "      <td>-0.489113</td>\n",
       "      <td>-0.947570</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>935 rows × 164 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id       date    ret_eom     gvkey  iid excntry  \\\n",
       "1370     comp_144498_01C 2005-02-28 2005-02-28  144498.0  01C     CAN   \n",
       "2147          crsp_19227 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
       "3145          crsp_71985 2005-02-28 2005-02-28    9919.0   01     USA   \n",
       "5830          crsp_88258 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
       "6131          crsp_89139 2005-02-28 2005-02-28       NaN  NaN     USA   \n",
       "...                  ...        ...        ...       ...  ...     ...   \n",
       "1383497   comp_009919_01 2025-02-28 2025-02-28    9919.0   01     USA   \n",
       "1386690   comp_009919_01 2025-03-31 2025-03-31    9919.0   01     USA   \n",
       "1389872   comp_009919_01 2025-04-30 2025-04-30    9919.0   01     USA   \n",
       "1393041   comp_009919_01 2025-05-31 2025-05-31    9919.0   01     USA   \n",
       "1396196   comp_009919_01 2025-06-30 2025-06-30    9919.0   01     USA   \n",
       "\n",
       "         stock_ret  year  month  char_date  ...  age       qmj  qmj_prof  \\\n",
       "1370     -0.395696  2005      2   20050131  ...   42       NaN       NaN   \n",
       "2147     -0.111594  2005      2   20050131  ...  124       NaN       NaN   \n",
       "3145     -0.050633  2005      2   20050131  ...  265  0.438271 -0.346311   \n",
       "5830     -0.090909  2005      2   20050131  ...  265       NaN       NaN   \n",
       "6131      0.269888  2005      2   20050131  ...   42       NaN       NaN   \n",
       "...            ...   ...    ...        ...  ...  ...       ...       ...   \n",
       "1383497  -0.161589  2025      2   20250131  ...  505 -0.960044 -0.637429   \n",
       "1386690  -0.192615  2025      3   20250228  ...  506 -0.937657 -0.629447   \n",
       "1389872   0.045735  2025      4   20250331  ...  507 -0.947467 -0.639770   \n",
       "1393041   0.246311  2025      5   20250430  ...  508 -1.181943 -0.537743   \n",
       "1396196   0.195424  2025      6   20250530  ...  509 -1.177438 -0.538941   \n",
       "\n",
       "         qmj_growth  qmj_safety  datadate  tic  conm  cusip  cik  \n",
       "1370            NaN         NaN       NaT  NaN   NaN    NaN  NaN  \n",
       "2147            NaN   -1.020464       NaT  NaN   NaN    NaN  NaN  \n",
       "3145       0.074514    1.153051       NaT  NaN   NaN    NaN  NaN  \n",
       "5830            NaN   -1.606884       NaT  NaN   NaN    NaN  NaN  \n",
       "6131            NaN   -1.355561       NaT  NaN   NaN    NaN  NaN  \n",
       "...             ...         ...       ...  ...   ...    ...  ...  \n",
       "1383497    0.172076   -1.030098       NaT  NaN   NaN    NaN  NaN  \n",
       "1386690    0.194402   -1.015891       NaT  NaN   NaN    NaN  NaN  \n",
       "1389872    0.193889   -1.014075       NaT  NaN   NaN    NaN  NaN  \n",
       "1393041   -0.500379   -0.944170       NaT  NaN   NaN    NaN  NaN  \n",
       "1396196   -0.489113   -0.947570       NaT  NaN   NaN    NaN  NaN  \n",
       "\n",
       "[935 rows x 164 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for NaN values in the merged columns\n",
    "merged_columns = ['tic', 'conm', 'cusip', 'cik']\n",
    "\n",
    "# Find rows where all merged columns are NaN\n",
    "nan_rows = merged1[merged1[merged_columns].isna().all(axis=1)]\n",
    "\n",
    "if nan_rows.empty:\n",
    "    print(\"No rows in merged1 have NaN for all the merged columns.\")\n",
    "else:\n",
    "    print(f\"Rows with NaN for all merged columns: {len(nan_rows)}\")\n",
    "    display(nan_rows)  # Use display() in a notebook to inspect the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa583391-4a4b-47d0-894e-d2e3d6bae117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dates in raw have a match in mappings3.\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'date' in raw and 'datadate' in mappings3 are in datetime format\n",
    "raw['date'] = pd.to_datetime(raw['date'])\n",
    "mappings3['datadate'] = pd.to_datetime(mappings3['datadate'])\n",
    "\n",
    "# Find non-overlapping dates in raw\n",
    "non_overlapping_dates = set(raw['date']) - set(mappings3['datadate'])\n",
    "\n",
    "# Print the results\n",
    "if non_overlapping_dates:\n",
    "    print(f\"Number of non-overlapping dates in raw: {len(non_overlapping_dates)}\")\n",
    "    print(\"Non-overlapping dates:\")\n",
    "    print(non_overlapping_dates)\n",
    "else:\n",
    "    print(\"All dates in raw have a match in mappings3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffd10895-0364-421d-9faf-3c17a64f0519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching rows found for gvkey=144498, date=2005-02-28, iid=01C.\n"
     ]
    }
   ],
   "source": [
    "# Function to check specific rows in mappings3 based on given values for gvkey, date, and iid\n",
    "def check_specific_rows(mappings3, gvkey, date, iid):\n",
    "    # Ensure 'datadate' is in datetime format\n",
    "    mappings3['datadate'] = pd.to_datetime(mappings3['datadate'])\n",
    "\n",
    "    # Filter rows based on the given values\n",
    "    specific_rows = mappings3[\n",
    "        (mappings3['gvkey'] == gvkey) &\n",
    "        (mappings3['datadate'] == pd.to_datetime(date)) &\n",
    "        (mappings3['iid'] == iid)\n",
    "    ]\n",
    "\n",
    "    # Check if any rows match\n",
    "    if not specific_rows.empty:\n",
    "        print(f\"Found {len(specific_rows)} matching rows for gvkey={gvkey}, date={date}, iid={iid}:\")\n",
    "        display(specific_rows)  # Use display() in a notebook to inspect the rows\n",
    "    else:\n",
    "        print(f\"No matching rows found for gvkey={gvkey}, date={date}, iid={iid}.\")\n",
    "\n",
    "# Example usage:\n",
    "# Replace the values below with the ones you want to check\n",
    "check_specific_rows(mappings3, gvkey=144498, date='2005-02-28', iid='01C')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
